{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Pengenalan Spark DataFrames"
      ],
      "metadata": {
        "id": "yufagub8x0j3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEZ91aB1xr2y"
      },
      "outputs": [],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('HandsOnPertemuan6').getOrCreate()\n",
        "\n",
        "data = [('James', 'Sales', 3000),\n",
        "        ('Michael', 'Sales', 4600),\n",
        "        ('Robert', 'Sales', 4100),\n",
        "        ('Maria', 'Finance', 3000)]\n",
        "columns = ['EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "FHvVqByYx8yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tugas 1: Buat DataFrame sederhana di Spark dan eksplorasi beberapa fungsi dasar yang tersedia."
      ],
      "metadata": {
        "id": "K3w4R7Wjx_u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"HandsOnPertemuan6\").getOrCreate()\n",
        "\n",
        "data1 = [('Ferdi', 'FT', 100),\n",
        "        ('Maulana', 'Faperta', 90),\n",
        "        ('Ikhsan', 'FT', 80),\n",
        "        ('Sadi', 'FKIP', 95)]\n",
        "\n",
        "columns1 = ['Nama', 'Fakultas', 'Nilai']\n",
        "\n",
        "df = spark.createDataFrame(data1, schema=columns1)\n",
        "df.show() #menampilkan semua data\n",
        "\n",
        "df.printSchema() #menampilkan struktur data\n",
        "\n",
        "#mengurutkan data berdasarkan nilai tertinggi\n",
        "df.orderBy(df['Nilai'].desc()).show()"
      ],
      "metadata": {
        "id": "oeMiyntgyEr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Transformasi Dasar dengan DataFrames"
      ],
      "metadata": {
        "id": "KZ5ei99wyKr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('HandsOnPertemuan6').getOrCreate()\n",
        "\n",
        "data2 = [('James', 'Sales', 3000),\n",
        "        ('Michael', 'Sales', 4600),\n",
        "        ('Robert', 'Sales', 4100),\n",
        "        ('Maria', 'Finance', 3000)]\n",
        "columns2 = ['EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "df = spark.createDataFrame(data2, schema=columns2)\n",
        "df.show()\n",
        "\n",
        "df.select('EmployeeName', 'Salary').show()\n",
        "df.filter(df['Salary'] > 3000).show()\n",
        "df.groupBy('Department').avg('Salary').show()"
      ],
      "metadata": {
        "id": "x3S8YdS1yPvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tugas 2: Gunakan operasi filter, select, groupBy untuk mengekstrak informasi dari data, serta lakukan agregasi data untuk mendapatkan insight tentang dataset menggunakan perintah seperti mean, max, sum."
      ],
      "metadata": {
        "id": "1AxUoKqTyTUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, max, mean, sum\n",
        "\n",
        "spark = SparkSession.builder.appName('HandsOnPertemuan6').getOrCreate()\n",
        "\n",
        "data3 = [('James', 'Sales', 3000),\n",
        "         ('Michael', 'Sales', 4600),\n",
        "         ('Robert', 'Sales', 4100),\n",
        "         ('Maria', 'Finance', 3000)]\n",
        "\n",
        "columns3 = ['EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "df = spark.createDataFrame(data3, schema=columns3)\n",
        "df.show()\n",
        "\n",
        "# 1. Select kolom tertentu (EmployeeName dan Salary)\n",
        "df.select('EmployeeName', 'Salary').show()\n",
        "\n",
        "# 2. Filter data karyawan yang gajinya lebih dari 3000\n",
        "df.filter(df['Salary'] > 3000).show()\n",
        "\n",
        "# 3. Menghitung rata-rata gaji per departemen\n",
        "df.groupBy('Department').avg('Salary').show()\n",
        "\n",
        "# 4. Menghitung total gaji per departemen\n",
        "df.groupBy('Department').sum('Salary').show()\n",
        "\n",
        "# 5. Menghitung gaji maksimum di seluruh departemen\n",
        "df.groupBy('Department').max('Salary').show()\n",
        "\n",
        "# 6. Menghitung rata-rata gaji di seluruh dataset\n",
        "df.select(mean('Salary')).show()\n",
        "\n",
        "# 7. Menghitung total gaji di seluruh dataset\n",
        "df.select(sum('Salary')).show()"
      ],
      "metadata": {
        "id": "tkfTIKZtyddC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Bekerja dengan Tipe Data Kompleks"
      ],
      "metadata": {
        "id": "VCgyTKLAyhsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, max, mean, sum\n",
        "\n",
        "spark = SparkSession.builder.appName('HandsOnPertemuan6').getOrCreate()\n",
        "\n",
        "data4 = [('James', 'Sales', 3000),\n",
        "         ('Michael', 'Sales', 4600),\n",
        "         ('Robert', 'Sales', 4100),\n",
        "         ('Maria', 'Finance', 3000)]\n",
        "\n",
        "columns4 = ['EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "df = spark.createDataFrame(data4, schema=columns4)\n",
        "\n",
        "df = df.withColumn('SalaryBonus', df['Salary'] * 0.1)\n",
        "df = df.withColumn('TotalCompensation', df['Salary'] + df['SalaryBonus'])\n",
        "df.show()"
      ],
      "metadata": {
        "id": "K9HMiz0Fyk8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Operasi Data Lanjutan"
      ],
      "metadata": {
        "id": "Y-I0b6Qjy3cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tugas 4: Implementasikan window function untuk menghitung running totals atau rangkings."
      ],
      "metadata": {
        "id": "Bm4ODwGly-H1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName('HandsOnPertemuan6').getOrCreate()\n",
        "\n",
        "data5 = [('James', 'Sales', 3000),\n",
        "         ('Michael', 'Sales', 4600),\n",
        "         ('Robert', 'Sales', 4100),\n",
        "         ('Maria', 'Finance', 3000)]\n",
        "\n",
        "columns5 = ['EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "df = spark.createDataFrame(data5, schema=columns5)\n",
        "\n",
        "windowSpec = Window.partitionBy('Department').orderBy('Salary')\n",
        "df.withColumn('Rank', F.rank().over(windowSpec)).show()"
      ],
      "metadata": {
        "id": "s3x0Mpygy7GG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}